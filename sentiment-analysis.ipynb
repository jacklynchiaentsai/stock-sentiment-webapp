{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis\n",
    "Natural Language Processing project that uses financial news headlines from finviz (https://finviz.com) to visualize and compare stock prospects for selected tickers over time   \n",
    "\n",
    "The Sentiment Analysis Model is trained under 5 different approaches below, and the one with the best accuracy is selected as the final model.\n",
    "- MLP (Multi Layer Perceptron)\n",
    "- CNN (Convolutional Neural Netwrok)\n",
    "- LSTM\n",
    "- GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Set\n",
    "Sentiment Analysis data for Financial News (https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news)   \n",
    "This dataset (FinancialPhraseBank) contains the sentiments for financial news headlines from the perspective of a retail investor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file (debugging encoding)\n",
    "colnames=['label', 'headline'] \n",
    "df = pd.read_csv('./data/all-data.csv',delimiter=',',encoding='latin-1', names=colnames, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pro = df['label'].value_counts()\n",
    "print(cnt_pro)\n",
    "print(type(cnt_pro.index))\n",
    "print(type(cnt_pro.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing data\n",
    "cnt_pro = df['label'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "# Create the bar plot and store the ax object\n",
    "ax = sns.barplot(x=cnt_pro.index, y=cnt_pro.values, alpha=0.8)\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for p in ax.patches:\n",
    "    height = int(p.get_height())  # Convert the height to an integer\n",
    "    ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "    \n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('sentiment', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processsing\n",
    "- converting label to numeric (postive: 1, neutral: 0, negative: -1)\n",
    "- cleaning text: tokenizing, removing punctuation, digits, convert to lowercase\n",
    "- converting dataframe to numpy arrays\n",
    "- fixing input data to tensors with same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert label to numeric\n",
    "sentiment  = {'positive': 1,'neutral': 0,'negative':-1} \n",
    "\n",
    "df.label = [sentiment[item] for item in df.label] \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert headline to list of strings\n",
    "# removing punctuation\n",
    "punctstr = string.punctuation\n",
    "punctstr = punctstr.replace('.','') # don't want to remove periods because they might represent decimal points\n",
    "print(punctstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text: puncuation, convert to lowercase\n",
    "punctdigstr = string.punctuation\n",
    "punctdigstr += \"0123456789\"\n",
    "\n",
    "def cleanText(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', punctdigstr))\n",
    "    text = text.replace(' s ', ' ') # removing 's\n",
    "\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "df['headline'] = df['headline'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[:20].headline)\n",
    "\n",
    "max_length = df['headline'].apply(len).max()\n",
    "print(\"Length of the longest list:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for determining the input_dim of the embedding layers later on\n",
    "# Concatenate all lists in the 'headline' column to create a single list containing all words\n",
    "all_words = [word for sublist in df['headline'] for word in sublist]\n",
    "\n",
    "# Calculate the vocabulary size, which is the total number of unique words\n",
    "vocabulary_size = len(set(all_words))\n",
    "\n",
    "print(\"Vocabulary size:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "max_words = vocabulary_size  # Set the maximum number of words in your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')  # <OOV> for out-of-vocabulary words\n",
    "\n",
    "# Fit the tokenizer on text data\n",
    "tokenizer.fit_on_texts(df['headline'])\n",
    "\n",
    "# Get the word-to-index mapping\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert each headline into a sequence of word indices\n",
    "sequences = tokenizer.texts_to_sequences(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_index_library.json', 'w') as f:\n",
    "    json.dump(word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences)\n",
    "print(type(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the 'headline' column containing the sequences\n",
    "df_headline_tokenized = pd.DataFrame({'headline': sequences})\n",
    "\n",
    "print(df_headline_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and testing data (80% - 20%)\n",
    "X = df_headline_tokenized  # Features\n",
    "y = df['label']              # Target\n",
    "\n",
    "# Set the proportion for training and testing (e.g., 80% for training, 20% for testing)\n",
    "test_size = 0.2\n",
    "\n",
    "# Split the data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the training and testing data into numpy arrays\n",
    "X_train= X_train['headline'].values\n",
    "X_test = X_test['headline'].values\n",
    "\n",
    "print(X_train)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the training and testing data into numpy arrays\n",
    "# y is a pandas series so require different conversion\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "print(y_train)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train, y_test into one-hot encoded format\n",
    "num_classes = 3\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert headline list to tensor and convert with same length (50)\n",
    "max_words = 50\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_words, dtype=object)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape: \",  X_train.shape)\n",
    "print(\"X_test.shape: \" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging: need to convert all values to same type\n",
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "X_test = np.asarray(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Vectorization\n",
    "Purpose:\n",
    "- map text to geometrical space, using space to describe relationship between text \n",
    "- RNN, LSTM, and GRU 's input and output data are all vectors\n",
    "\n",
    "Implementation: Adding Embedding layer with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example that will be implemented likewise in every model\n",
    "# Embedding must be the first layer of sequential model\n",
    "model = Sequential()\n",
    "# setting output dimension to 100 initially\n",
    "model.add(Embedding(vocabulary_size, 32, input_length=max_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLP for Sentiment Analysis\n",
    "observed how people built their MLP archtitecture in similar sentiment analysis projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3  # Three classes: positive, neutral, and negative\n",
    "MLP_model = Sequential()\n",
    "MLP_model.add(Embedding(vocabulary_size, 32, input_length=max_words))\n",
    "MLP_model.add(Dropout(0.25)) # 25% of the units will be set to 0 during training\n",
    "MLP_model.add(Flatten())    # flatten to 1D vector\n",
    "MLP_model.add(Dense(256, activation=\"relu\")) # relu: non-linearity\n",
    "MLP_model.add(Dropout(0.25))\n",
    "MLP_model.add(Dense(units=num_classes, activation='softmax'))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "# For multi-class classification, 'categorical_crossentropy' is used.\n",
    "MLP_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "MLP_history = MLP_model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=102, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model performance with testing data\n",
    "MLP_loss, MLP_accuracy = MLP_model.evaluate(X_test, y_test)\n",
    "print(\"testing dataset's accuracy = {:.2f}\".format(MLP_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNN for Sentiment Analysis\n",
    "RNN has memory over sequential data, can treat each headline as a sequence data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "RNN_model = Sequential()\n",
    "RNN_model.add(Embedding(vocabulary_size, 32, input_length=max_words))\n",
    "RNN_model.add(Dropout(0.25))\n",
    "# Set return_sequences=True to ensure the output is 3D (batch_size, timesteps, units)\n",
    "RNN_model.add(SimpleRNN(32))\n",
    "RNN_model.add(Dropout(0.25))\n",
    "RNN_model.add(Dense(units=num_classes, activation='softmax'))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "# For multi-class classification, 'categorical_crossentropy' is used.\n",
    "RNN_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "RNN_history = RNN_model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=102, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model performance with testing data\n",
    "RNN_loss, RNN_accuracy = RNN_model.evaluate(X_test, y_test)\n",
    "print(\"testing dataset's accuracy = {:.2f}\".format(RNN_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM for Sentiment Analysis\n",
    "LSTM is an improvisation of RNN's vanishing gradient problem, which includes long term memory via cell state, can analyze words with long time step gaps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
